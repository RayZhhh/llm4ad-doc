{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc451e17cac7454",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluator and SecureEvaluator\n",
    "\n",
    "> This tutorial demonstrates how to evaluate a function using user-specified evaluator. The evaluation process is protected in a SecureEvaluator to prevent \"very bad code\" (e.g., with an endless loop, raise unexpected exceptions, consume too much memory, remain an unkilled subprocess, ...)\n",
    "\n",
    "## Evaluator class\n",
    "The Evaluator class (an abstract class) is an user interface. The user should define a child class of `Evaluator` (Extend the Evaluator class). \n",
    "\n",
    "### Initialization of the Evaluator class.\n",
    "By passing the respective argument to the Evaluator, the user can specify if to use numba acceleration, use protected division, timeout second for code execution. Details about all arguments can be found in base_package/evaluate section of this doc.\n",
    "\n",
    "### Implementation of the evaluate_program function\n",
    "The user should override the `evaluate_program` function in the Evaluator class (where the `evaluate_program` function remains unimplemented). The evaluate_program function evaluate the algorithm and gives a score of it. If the user think the algorithm is infeasible/invalid/illegal, the user should return `None`. Otherwise, a int/float value or a value that is comparable (which may implements `>` operator between the them) is desired.\n",
    "\n",
    "The first argument of the function is a `program_str`, which is a `str` type of algorithm to be evaluated. If you set the `use_numba_accelerate` or similar settings to `True` in the initialization, you will obtain a `str` typed function that has been modified. This `str` is provided to let you:\n",
    "\n",
    "- Compile and execute the code with your own requirement.\n",
    "- Taking the length or other features of the code in consideration.\n",
    "- Other usage such as calculate the \"novelty\" of the code, or retrieve if the code has been evaluated before.\n",
    "\n",
    "The second argument of the function is a `callable_func`, which is a executable object. You can simply call (invoke) it by passing arguments to `callable_func`. Such as `callable_function(arg0, arg1)`.\n",
    "\n",
    "## SecureEvaluator class\n",
    "This class is going to perform secure evaluation based on the user-specified `Evaluator` instance. This tutorial will show few examples about the features of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0cdfbcea8db39",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "Below are examples on how to use these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:42.555405Z",
     "start_time": "2024-10-21T14:58:37.943250Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "from llm4ad.base import Evaluator, SecureEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f0f2b98d2e430",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The user should implement 'llm4ad.base.Evaluator' class and override the 'evaluate_program' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ba8b132549f719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:42.568410Z",
     "start_time": "2024-10-21T14:58:42.561938Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyEvaluator(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_numba_accelerate=True,  # try to set to 'False' and execute \n",
    "            use_protected_div=True,  # avoid divided by 0\n",
    "            timeout_seconds=5, \n",
    "        )\n",
    "    \n",
    "    # the user should override this function.\n",
    "    def evaluate_program(self, program_str: str, callable_func: callable, **kwargs) -> Any | None:\n",
    "        # we consider a \"dummy evaluation\" for the function:\n",
    "        # we call (invoke) the function and get its return value as the score of this function\n",
    "        score = callable_func()\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7877655aada376",
   "metadata": {},
   "source": [
    "We create an evaluator instance and encapsulate the instance to a SecureEvaluator, so that we can perform a secure evaluation. We also set the evaluator to debug mode to visualize the function to be evalauted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebf2203f9a90e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:42.657716Z",
     "start_time": "2024-10-21T14:58:42.654170Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = SecureEvaluator(evaluator=MyEvaluator(), debug_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e6533df9ee2b2",
   "metadata": {},
   "source": [
    "Here we prepare a simple demo of evaluated algorithm (in str)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7056a51766f59da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:42.671930Z",
     "start_time": "2024-10-21T14:58:42.668326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program = \"\"\"\n",
    "import random\n",
    "\n",
    "def f():\n",
    "    return random.random() / random.random()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233f3c165072ff5",
   "metadata": {},
   "source": [
    "Invoke `evaluate_program` function to evaluate the program. Please note that since the user set the argument `use_numba_accelerate=True` in the `MyEvaluator`, the evaluated program should be wrapped with a `@numba.jit()` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd8e5bf136a3201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:44.374071Z",
     "start_time": "2024-10-21T14:58:42.684059Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: evaluated program:\n",
      "import numba\n",
      "import random\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def f():\n",
      "    return _protected_div(random.random(), random.random())\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def _protected_div(x, y, delta=1e-05):\n",
      "    return x / (y + delta)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9887838167313623"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = evaluator.evaluate_program(program)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d77cc8f587bb53",
   "metadata": {},
   "source": [
    "Assuming that we have obtained a program within a `while True` loop, let's see if the secure evaluator can terminate the evaluation after the `timeout_seconds` specified by the user in `MyEvaluator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318acb26920a4b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:44.390498Z",
     "start_time": "2024-10-21T14:58:44.387478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program = \"\"\"\n",
    "import random\n",
    "\n",
    "def f():\n",
    "    while True:\n",
    "        pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3fbf6d59fe9e9",
   "metadata": {},
   "source": [
    "Evaluate the program. We can observe from the debug information that the evaluation of the program exceeds 5 seconds, thus is terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f16d7f44e183f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:49.448368Z",
     "start_time": "2024-10-21T14:58:44.404547Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: evaluated program:\n",
      "import numba\n",
      "import random\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def f():\n",
      "    while True:\n",
      "        pass\n",
      "\n",
      "@numba.jit(nopython=True)\n",
      "def _protected_div(x, y, delta=1e-05):\n",
      "    return x / (y + delta)\n",
      "\n",
      "DEBUG: the evaluation time exceeds 5s.\n"
     ]
    }
   ],
   "source": [
    "score = evaluator.evaluate_program(program)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8808398d57b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T14:58:49.478450Z",
     "start_time": "2024-10-21T14:58:49.475757Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
